---
layout: post
title: "Sample Review of DRL"
categories: study
author: "Yizheng Huang"
---

最近因为项目和论文的关系需要用到一些 Deep Reinforcement Learning 的知识，于是快速把 DRL 的一些基本算法和思想过了一遍。之前赶时间寥寥草草地写了七八页纸，现在因为 COVID-19 导致各种 DDL 推迟了以后便有了一些空闲时间，觉得还是记录在博客比较好。个人觉得 RL 这个东西思想是很精妙的，但如果只是要了解一些比较粗浅的东西，学习成本很低，完全可以几天内掌握个大概。

由于我比较懒，这篇博客主要是写给自己看的，可能有些地方不会解释得太清楚 : )

### DRL 中的 Policy Gradient

强化学习实际上是一个机器与环境不断互动和学习的过程，其中包括几个重要的组成部分:

-   Agent: 与环境互动的智能体
-   Environment: 与智能体交互的环境
-   Reward Function: 环境给予智能体反馈的方式

举个例子，比如使用强化学习玩游戏，那么理论上的一个流程就是：

-   初始化一个 agent
-   agent 接收环境所给的第一个界面，也是输入第一个 state: $$ s_1 $$
-   agent 给出一个对应的反应：$$ a_1 $$
-   环境接收 $$ a_1 $$ 给出对应的 $$ s_2 $$

重复上述流程直到游戏结束。

我们认为从游戏开始到游戏结束是一个 episode，用 $$ \tau $$ 表示。然后在这个玩游戏的过程中，举个例子：假设这个游戏是我们熟知的雷电（飞机大战游戏）。用户需要操作飞机左右移动以避开飞来的陨石等障碍，同时又要主动出击才能获得比较高的分数。这个我方战斗机便可以看作强化学习中的 agent，周围的陨石，敌机等无法控制（含有随机性）的东西就是与我们 agent 交互的环境。

为了让我们的 agent 在玩游戏的过程中逐渐掌握游戏的技巧，我们需要设计 Reward Function, 也就是设计一个反馈机制。其实游戏本身是含有这样的反馈机制的，比如击落一架敌方战斗机可以获得多少分，吃到补给可以获得多少分，被子弹击中扣多少分这样。agent 做出的每一步，或多或少都在改变着最终的游戏结果。

我们把整个 episode 最终获得的分数用 reward function 表示为 

$$ R(\tau) = \{ r_1 + r_2 + r_3 + ... + r_n \} $$

深度强化学习，之所以称为深度强化学习，是因为我们的 agent 实际上是一个 DNN，给定某个 state 输入，针对这个输入输出对应的 action，学习的过程实际上就是在 update 这个 DNN 的参数，使得最终一个 episode 下来全局的 reward function $$ R(\tau) $$ 可以达到最大值。

其中，我们把一个 agent 进行玩游戏的策略称为一个 policy, 用 $$ \theta $$ 表示，不同的 $$ \theta $$ 表示不同的游戏策略（不同的 agent）, 我们要做的就是求给定 $$ \theta $$ 的 $$ R_{\theta} $$ 的最大值, 这里我们可以用梯度增加的方式计算。

$$ \theta \leftarrow \theta + \eta \nabla R $$

为了准确更新神经网络的参数，我们需要尽可能多的获取一些游戏数据，在一个相同的 policy 下，我们可能会进行非常多场游戏。所以计算 $$ t $$ 场游戏的平均 reward 就是：

$$ \overline{R_{\theta}} = \sum_{\tau} R(\tau) p_{\theta} (\tau) $$

对 $$ \theta $$ 求梯度：

$$
\begin{align}

\nabla \overline{R_{\theta}} & = \sum_{\tau} R(\tau) \nabla p_{\theta} (\tau) \\
& = \sum_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\ 
& = \sum_{\tau} R(\tau) p_{\theta} \nabla \log p_{\theta}(\tau) \\
& = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left [ R(\tau) \nabla \log p_{\theta}(\tau) \right ] \\ 
& \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^{n}) \nabla \log p_{\theta}(a_t^n | s_t^n)

\end{align}
$$

为了方便实现最终将式子写成了上述这种形式，其中 $$ R(\tau^{n}) $$ 是第 n 个 episode 的 reward 总和，$$ T_n $$ 代表的意思是在第 n 个 episode 里面，总共有 $$ T_n $$ 个 step (一个 step 定义为给定一个 state s, agent 做出一个反应 a)。

这个式子是非常好理解的，为了让最终的 policy gradient 有最大值，当某个 step 发生的那个 $$ \tau $$ 中有相对较大的 $$ R(\tau) $$，我们就要增加其出现的概率，反之，如果 reward 我们就要减小这个操作所出现的概率。

上述公式中用了一个近似，在给定分布求期望的过程中：

$$ \mathbb{E}_{x \sim p} \left [ f(x) \right ] \approx \frac{1}{N} \sum_{i=1}^{N} f(x^i) $$ 

这里的 N 越大，实际上相当于在 p(x) 分布上 sample 到的值越多，结果也就越接近。


### On-Policy 的学习方式

理解了上述原理，之后要做的无非就是更新神经网络，On-Policy 的意思就是：与环境交互学习的 agent 和被动更新的 agent 是同一个。具体的流程可以表示为：

- agent 先初始化，并且与环境做互动
- 在互动的过程中我们 sample 非常多的数据（Important Sampling）
- 在积累了 m 个 $$ \tau $$ 的数据以后，我们用这么多数据去 update agent policy
- 把用过的数据扔掉
- 使用新的 agent 与环境继续互动，优化 reward function
- ...

显而易见，on-policy 的方式是存在一定问题的，比如进行飞机大战的游戏，输入 DNN 的 state 是用 image 表示的，训练 -> sample -> 训练这样的方式非常耗时。



<!-- 
另外，我们可以用另外一个分布 $$ q(x) $$ 来估计 $$ p(x) $$:

$$
\begin{align}

\mathbb{E}_{x \sim p} \left [ f(x) \right ] \approx \frac{1}{N} \sum_{i=1}^{N} f(x^i) \\
& = \int f(x)p(x) dx = \int \frac{f(x)p(x)}{q(x)} \cdot q(x) \\
& = \mathbb{E}_{x \sim q} \left [ \frac{f(x)p(x)}{q(x)} \right ] \\ 

\end{align}
$$

 -->
