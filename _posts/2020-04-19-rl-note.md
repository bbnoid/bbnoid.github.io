---
layout: post
title: "Sample Review of DRL"
categories: study
author: "Yizheng Huang"
---

最近因为项目和论文的关系需要用到一些 Deep Reinforcement Learning 的知识，于是快速把 DRL 的一些基本算法和思想过了一遍。之前由于赶时间寥寥草草地写了七八页纸，现在因为 COVID-19 导致各种 DDL 推迟了以后，莫名其妙多出了一些空闲实践，觉得还是记录在博客比较好。个人觉得 RL 这个东西，思想是很精妙的，但如果只是要了解一些比较粗浅的东西，学习成本很低，完全可以几天内掌握个大概。

由于我比较懒，这篇博客主要是写给自己看的，可能有些地方不会解释得太清楚 : )

### DRL 中的 Policy Gradient

强化学习实际上是一个机器与环境不断互动和学习的过程，其中包括几个重要的组成部分:

-   Agent: 与环境互动的智能体
-   Environment: 与智能体交互的环境
-   Reward Function: 环境给予智能体反馈的方式

举个例子，比如使用强化学习玩游戏，那么理论上的一个流程就是：

-   初始化一个 agent
-   agent 接收环境所给的第一个界面，也是输入第一个 state: $$ s_1 $$
-   agent 给出一个对应的反应：$$ a_1 $$
-   环境接收 $$ a_1 $$ 给出对应的 $$ s_2 $$

重复上述流程直到游戏结束。我们认为从游戏开始到游戏结束是一个 episode，用 $$ \tau $$ 表示。然后在这个玩游戏的过程中呢，举个例子：假设这个游戏是我们熟知的雷电（打飞机游戏）。用户需要操作飞机左右移动以避开飞来的陨石等障碍，同时又要主动出击，击落敌机才能获得比较高的分数。那么这个我方战斗机便可以看作 agent，周围的陨石，敌机等无法控制（含有随机性）的东西就是与我们 agent 交互的环境。

为了让我们的 agent 在玩游戏的过程中逐渐掌握游戏的技巧，我们需要设计 Reward Function, 也就是设计一个反馈机制。其实游戏本身是含有这样的反馈机制的，比如击落一架敌方战斗机可以获得多少分，吃到补给可以获得多少分，被子弹击中扣多少分这样。agent 做出的每一步，或多或少都在改变着最终的游戏结果。

我们把整个 episode 最终获得的分数用 reward function 表示为 $$ R(\tau) = \{ r_1 + r_2 + r_3 + ... + r_n \} $$。深度强化学习，之所以称为深度强化学习，是因为我们的 agent 实际上是一个 DNN。给定某个 state 输入，针对这个输入输出对应的 action，学习的过程实际上就是在 update 这个 DNN 的参数，使得最终一个 episode 下来，并且 overall 的 reward $$ R(\tau) $$ 可以达到最大。

其中，我们把一个 agent 进行玩游戏的策略称为一个 policy, 用 $$ \theta $$ 表示，不同的 $$ \theta $$ 表示不同的游戏策略, 我们要做的就是求给定 $$ \theta $$ 的 $$ R_{\theta} $$ 的最大值, 这里我们自然可以梯度增加的方式计算。

$$ \theta \leftarrow \theta + \eta \nabla R $$

为了准确更新神经网络的参数，我们想尽可能多的获取一些游戏数据，在一个相同的 policy 下，我们可能会进行非常多场游戏。所以计算 $$ t $$ 场游戏的平均 reward 就是：

$$ \overline{R_{\theta}} = \sum_{\tau} R(\tau) p_{\theta} (\tau) $$

对 $$ \theta $$ 求梯度：

\begin{align}
\nabla \overline{R_{\theta}} = \sum_{\tau} R(\tau) \nabla p_{\theta} (\tau) \\
& = \sum_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\ 
& = \sum_{\tau} R(\tau) p_{\theta} \nabla \log p_{\theta}(\tau) \\
& = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left [ R(\tau) \nabla \log p_{\theta}(\tau) \right ] \\ 
& \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^{n}) \nabla \log p_{\theta}(a_t^n | s_t^n)
\end{align}
