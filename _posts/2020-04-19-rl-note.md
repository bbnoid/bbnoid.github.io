---
layout: post
title: "Simple Review of DRL"
categories: study
author: "Yizheng Huang"
---

最近因为项目和论文的关系需要用到一些 Deep Reinforcement Learning 的知识，于是快速把 DRL 的一些基本算法和思想过了一遍。之前赶时间寥寥草草地写了七八页纸，现在因为 COVID-19 导致各种 DDL 推迟了以后便有了一些空闲时间，觉得还是记录在博客比较好。个人觉得 RL 这个东西思想是很精妙的，但如果只是要了解一些比较粗浅的东西，学习成本很低，完全可以几天内掌握个大概。

由于我比较懒，这篇博客主要是写给自己看的，可能有些地方不会解释得太清楚 : )

### DRL 中的 Policy Gradient

强化学习实际上是一个机器与环境不断互动和学习的过程，其中包括几个重要的组成部分:

-   Agent: 与环境互动的智能体
-   Environment: 与智能体交互的环境
-   Reward Function: 环境给予智能体反馈的方式

举个例子，比如使用强化学习玩游戏，那么理论上的一个流程就是：

-   初始化一个 agent
-   agent 接收环境所给的第一个界面，也是输入第一个 state: $$ s_1 $$
-   agent 给出一个对应的反应：$$ a_1 $$
-   环境接收 $$ a_1 $$ 给出对应的 $$ s_2 $$

重复上述流程直到游戏结束。

我们认为从游戏开始到游戏结束是一个 episode，用 $$ \tau $$ 表示。然后在这个玩游戏的过程中，举个例子：假设这个游戏是我们熟知的雷电（飞机大战游戏）。用户需要操作飞机左右移动以避开飞来的陨石等障碍，同时又要主动出击才能获得比较高的分数。这个我方战斗机便可以看作强化学习中的 agent，周围的陨石，敌机等无法控制（含有随机性）的东西就是与我们 agent 交互的环境。

为了让我们的 agent 在玩游戏的过程中逐渐掌握游戏的技巧，我们需要设计 Reward Function, 也就是设计一个反馈机制。其实游戏本身是含有这样的反馈机制的，比如击落一架敌方战斗机可以获得多少分，吃到补给可以获得多少分，被子弹击中扣多少分这样。agent 做出的每一步，或多或少都在改变着最终的游戏结果。

我们把整个 episode 最终获得的分数用 reward function 表示为 

$$ R(\tau) = \{ r_1 + r_2 + r_3 + ... + r_n \} $$

深度强化学习，之所以称为深度强化学习，是因为我们的 agent 实际上是一个 DNN，给定某个 state 输入，针对这个输入输出对应的 action，学习的过程实际上就是在 update 这个 DNN 的参数，使得最终一个 episode 下来全局的 reward function $$ R(\tau) $$ 可以达到最大值。

其中，我们把一个 agent 进行玩游戏的策略称为一个 policy, 用 $$ \theta $$ 表示，不同的 $$ \theta $$ 表示不同的游戏策略（不同的 agent）, 我们要做的就是求给定 $$ \theta $$ 的 $$ R_{\theta} $$ 的最大值, 这里我们可以用梯度增加的方式计算。

$$ \theta \leftarrow \theta + \eta \nabla R $$

为了准确更新神经网络的参数，我们需要尽可能多的获取一些游戏数据，在一个相同的 policy 下，我们可能会进行非常多场游戏。所以计算 $$ t $$ 场游戏的平均 reward 就是：

$$ \overline{R_{\theta}} = \sum_{\tau} R(\tau) p_{\theta} (\tau) $$

对 $$ \theta $$ 求梯度：

$$
\begin{align}

\nabla \overline{R_{\theta}} & = \sum_{\tau} R(\tau) \nabla p_{\theta} (\tau) \\
& = \sum_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\ 
& = \sum_{\tau} R(\tau) p_{\theta}(\tau) \nabla \log p_{\theta}(\tau) \\
& = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left [ R(\tau) \nabla \log p_{\theta}(\tau) \right ] \\ 
& \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^{n}) \nabla \log p_{\theta}(a_t^n | s_t^n)

\end{align}
$$

为了方便实现最终将式子写成了上述这种形式，其中 $$ R(\tau^{n}) $$ 是第 n 个 episode 的 reward 总和，$$ T_n $$ 代表的意思是在第 n 个 episode 里面，总共有 $$ T_n $$ 个 step (一个 step 定义为给定一个 state s, agent 做出一个反应 a)。

这个式子是非常好理解的，为了让最终的 policy gradient 有最大值，当某个 step 发生的那个 $$ \tau $$ 中有相对较大的 $$ R(\tau) $$，我们就要增加其出现的概率，反之，如果 reward 的值太小我们就要减小这个操作所出现的概率。

上述公式中用了一个近似，在给定分布求期望的过程中：

$$ \mathbb{E}_{x \sim p} \left [ f(x) \right ] \approx \frac{1}{N} \sum_{i=1}^{N} f(x^i) $$ 

这里的 N 越大，实际上相当于在 p(x) 分布上 sample 到的值越多，结果也就越接近。

另外一个小技巧是：

$$
\nabla f(x) = f(x) \nabla \log f(x)
$$

所以，我们可以通过分子分母同时乘上一个 $$ p_{\theta}(\tau) $$ 将 $$ $$ p_{\theta}(\tau) $$ 中梯度运算中拿出来：

$$ 
\sum_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} = \sum_{\tau} R(\tau) p_{\theta}(\tau) \nabla \log p_{\theta}(\tau) $$


### 更精准的 Reward Function

在上述公式中，实际上存在着一些问题，其中最大的问题就是：该如何定义我们的 reward function $$ R_{\theta}(\tau) $$？如果仅仅是按照游戏的规则来，$$ R_{\theta}(\tau) $$ 是游戏中的每一步所产生的 reward 在整场游戏中的累加，在式子：

$$
\nabla \overline{R_{\theta}} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_n} R(\tau^{n}) \nabla \log p_{\theta}(a_t^n | s_t^n) $$

整个游戏下来，有些 action 是好的，有的是不好的，但是所有的 action 的概率前面都会被乘上同样的 weight: $$ R(\tau^{n}) $$，显然是不合理的。

那么如果我在给定某个 $$ s_t $$ agent 输出了 $$ s_t $$ ，实际上它并不会影响到 $$ a_t $$ 之前的那些情况，在 $$ s_t $$ 发生之前的 reward 实际上是和 $$ a_t $$ 无关的。

举个例子，一个简单的游戏我们玩了两场：

| State | $$ s_a $$ | $$ s_b $$ | $$ s_c $$ | 
|:--:|:--:|:--:|:--:|
| Action | $$ a_1 $$ | $$ a_2 $$ | $$ a_3 $$ | 
| Reward | +10 | +0 | -6 | 

其中 $$ R_1 = 4 $$

| State | $$ s_a $$ | $$ s_b $$ | $$ s_c $$ | 
|:--:|:--:|:--:|:--:|
| Action | $$ a_2 $$ | $$ a_2 $$ | $$ a_3 $$ | 
| Reward | -5 | +0 | -6 | 


$$ R_2 = -11 $$

那么在 $$ s_b $$ 执行 $$ a_2 $$ 在第一种游戏情况上就会被增加概率 (乘上 4)，而在第二种情况下同样的场景和操作就会被降低概率 (乘上 -11)，这是不科学的，第二场游戏之所以不好，是因为在 $$ (s_2, a_2) $$ 之前的 $$ (s_a, a_2) $$ 产生了 -5 的 reward，这个实际上和 $$ (s_2, a_2) $$ 是无关的。但是 $$ (s_2, a_2) $$ 之后的是和它有关的，$$ (s_c, a_3) $$ 可能正是要发生在 $$ (s_2, a_2) $$ 之后才会带来 -6 的 reward。


### On-Policy 到 Off-Policy

#### On-Policy 学习方式

理解了上述原理，之后要做的无非就是更新神经网络，On-Policy 的意思就是：与环境交互学习的 agent 和被动更新的 agent 是同一个。具体的流程可以表示为：

- agent 先初始化，并且与环境做互动
- 在互动的过程中我们 sample 非常多的数据
- 在积累了 m 个 $$ \tau $$ 的数据以后，我们用这么多数据去 update agent policy
- 把用过的数据扔掉
- 使用新的 agent 与环境继续互动，优化 reward function
- ...

显而易见，on-policy 的方式是存在一定问题的，比如进行飞机大战的游戏，输入 DNN 的 state 是用 image 表示的，训练 -> sample -> 训练 这样的方式非常耗时。并且一旦原有的 policy 更新了以后，

$$
\nabla \overline{R_{\theta}} = \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left [ R(\tau) \nabla \log p_{\theta}(\tau) \right ]
$$

上述梯度中分布 $$ p_{\theta}(\tau) $$ 就变了，之前在老的 policy 上面采样的数据就没用了，必须抛弃掉。

所以针对这种 On-Policy 研究人员希望能够在不影响 agent 与环境互动的前提下持续地对我们需要的 agent 进行更新。Off-Policy 应运而生，这里主要讲 PPO/TRPO 和 PPO2 两种 Off-Policy 的方法。

#### Important Sampling 

具体的操作方法就是使用 Important Sampling, 这个并不是 RL 里面独有的方法，简要来说就是用一个不同的分布 $$ q(x) $$ 去估计我们所需要的分布 $$ p(x) $$，在 Off-Policy 中体现为：我们想用另外一个 $$ {\theta}' $$ 去跟环境做互动，使用 $$ {\theta}' $$ 收集到的数据去训练我们想要的 $$ \theta $$，这个流程就像你让 agent 去看另外一个 agent 玩游戏，并从中学到游戏的方法。

这样，我们通过 $$ {\theta}' $$ 与环境互动获取到的数据可以被使用多次，不需要考虑 $$ \theta $$ 变化时数据就会失效的问题。

Important Sampling 中用另外一个分布 $$ q(x) $$ 来估计 $$ p(x) $$ 可以这样表示:

$$
\begin{align}

\mathbb{E}_{x \sim p} \left [ f(x) \right ] \approx \frac{1}{N} \sum_{i=1}^{N} f(x^i) \\
& = \int f(x)p(x) dx = \int \frac{f(x)p(x)}{q(x)} \cdot q(x) \\
& = \mathbb{E}_{x \sim q} \left [ \frac{f(x)p(x)}{q(x)} \right ] \\ 

\end{align}
$$

